# بخش ۳: تحلیل درخت تصمیم (Decision Tree)

## هدف
بررسی الگوریتم درخت تصمیم با معیار entropy برای عمق‌های ۲ تا ۱۰ و تحلیل Overfitting.

---

## پاسخ به سوالات تمرین

### نتایج آزمایش Decision Tree برای max_depth از 2 تا 10

| عمق (max_depth) | دقت آموزش (Train Acc) | دقت آزمون (Test Acc) |
|-----------------|----------------------|---------------------|
| 2 | 0.3278 | 0.2926 |
| 3 | 0.5060 | 0.4796 |
| 4 | 0.6650 | 0.5833 |
| 5 | 0.7828 | 0.6833 |
| 6 | 0.8735 | 0.7537 |
| 7 | 0.9339 | 0.8000 |
| **8** | **0.9697** | **0.8241** |
| 9 | 0.9928 | 0.8185 |
| 10 | 0.9984 | 0.8148 |

### بهترین عمق
**max_depth = 8** با دقت آزمون 0.8241 (82.41%)

---

### نقطه شروع Overfitting

**نقطه شروع Overfitting: عمق 9**

**تشخیص:**
- از عمق 9 به بعد، دقت آموزش همچنان افزایش می‌یابد (به 99.28%)
- اما دقت آزمون شروع به کاهش می‌کند (از 82.41% به 81.85%)
- شکاف بین دقت آموزش و آزمون بیشتر می‌شود
- این نشان‌دهنده این است که مدل داده‌های آموزش را "حفظ" می‌کند به جای "یادگیری" الگوها

---

### آیا افزایش عمق باعث بهبود مدل می‌شود؟

**پاسخ: خیر، لزوماً نه!**

**توضیح:**
1. **عمق کم (مثلاً 2-4) → Underfitting:**
   - مدل خیلی ساده است و نمی‌تواند الگوهای پیچیده را یاد بگیرد
   - دقت آموزش و آزمون هر دو پایین هستند
   - مدل توانایی کافی برای جداسازی کلاس‌ها ندارد

2. **عمق بهینه (مثلاً 8):**
   - تعادل بین پیچیدگی و سادگی مدل
   - بهترین Generalization (تعمیم‌پذیری) روی داده‌های جدید
   - دقت آزمون حداکثر است

3. **عمق زیاد (مثلاً 9-10) → Overfitting:**
   - مدل داده‌های آموزش را حفظ می‌کند (memorization)
   - روی داده‌های جدید ضعیف عمل می‌کند
   - دقت آموزش به 100% نزدیک می‌شود اما دقت آزمون کاهش می‌یابد
   - درخت قوانین خیلی خاص و غیرقابل تعمیم یاد می‌گیرد

**نتیجه:** افزایش عمق تا یک حد مشخص بهبود ایجاد می‌کند، اما بعد از آن منجر به Overfitting می‌شود.

---

## جزئیات فنی

### درخت تصمیم چیست؟
درخت تصمیم یک الگوریتم یادگیری ماشین است که:
- داده‌ها را با سوالات متوالی تقسیم می‌کند
- هر گره یک تصمیم بر اساس یک ویژگی است
- برگ‌ها برچسب نهایی را مشخص می‌کنند

### معیار Entropy
```
Entropy = -Σ(pi × log2(pi))
```
- اگر همه داده‌ها یک کلاس باشند: Entropy = 0
- اگر داده‌ها به طور مساوی توزیع شوند: Entropy حداکثر

## خروجی‌های کد
- `decision_tree_analysis.png`: نمودار دقت آموزش و آزمون
- `overfitting_analysis.png`: نمودار شاخص Overfitting

## نحوه اجرا
```bash
python decision_tree_analysis.py
```
