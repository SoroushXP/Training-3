# بخش ۲: تحلیل الگوریتم KNN

## هدف
بررسی الگوریتم K-Nearest Neighbors (KNN) برای مقادیر k از ۱ تا ۴۰ و یافتن مقدار بهینه k.

## الگوریتم KNN چیست؟
KNN یک الگوریتم یادگیری ماشین است که برای طبقه‌بندی یک نمونه جدید:
1. k نزدیک‌ترین همسایه را در داده‌های آموزش پیدا می‌کند
2. برچسب اکثریت همسایگان را به نمونه جدید اختصاص می‌دهد

## آزمایش انجام شده
- اجرای KNN برای k=1 تا k=40
- محاسبه دقت (Accuracy) و خطا برای هر مقدار k
- رسم نمودار دقت بر حسب k
- مقایسه نتایج با و بدون نرمال‌سازی

## تحلیل اثر مقدار k

### k کوچک (مثلاً k=1)
- **Overfitting**: مدل بیش از حد به داده‌های آموزش وفادار است
- **حساسیت بالا**: به نویز و داده‌های پرت حساس است
- **واریانس بالا، بایاس پایین**
- **تفسیر**: مدل فقط به یک همسایه نگاه می‌کند و ممکن است تصمیم اشتباه بگیرد

### k بزرگ (مثلاً k=40)
- **Underfitting**: مدل بیش از حد ساده می‌شود
- **تصمیم‌گیری کلی**: به همسایگان دور هم توجه می‌کند
- **واریانس پایین، بایاس بالا**
- **تفسیر**: تأثیر همسایگان بی‌ربط ممکن است نتیجه را خراب کند

### k بهینه
- تعادل بین Overfitting و Underfitting
- بهترین عملکرد روی داده‌های آزمون
- معمولاً با آزمایش و cross-validation پیدا می‌شود

## تفاوت نتایج با و بدون نرمال‌سازی

### چرا نرمال‌سازی مهم است؟
- KNN از فاصله اقلیدسی استفاده می‌کند:
  ```
  d(x, y) = √(Σ(xi - yi)²)
  ```
- ویژگی‌های با مقیاس بزرگ‌تر تأثیر بیشتری در محاسبه فاصله دارند
- نرمال‌سازی باعث می‌شود همه ویژگی‌ها تأثیر برابر داشته باشند

### در دیتاست Digits
- همه پیکسل‌ها در بازه [0, 16] هستند
- مقیاس ویژگی‌ها تقریباً یکسان است
- تفاوت زیادی بین حالت نرمال و غیرنرمال مشاهده نمی‌شود
- اما در دیتاست‌های دیگر این تفاوت می‌تواند چشمگیر باشد

## خروجی‌های کد
- `knn_analysis.png`: نمودار دقت و خطا بر حسب k
- نمایش بهترین k و دقت متناظر

## نتیجه‌گیری
- مقدار k بهینه تعادلی بین پیچیدگی و سادگی مدل ایجاد می‌کند
- انتخاب k مناسب نیازمند آزمایش است
- نرمال‌سازی برای الگوریتم‌های مبتنی بر فاصله توصیه می‌شود

## نحوه اجرا
```bash
python knn_analysis.py
```
